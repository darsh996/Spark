{"cells":[{"cell_type":"code","source":["# We will use pyspark library to use Apache Spark using Python\n# we will import SparkContext and Spark Cofiguration class from this library\n# to create/assign, remove, assign resources, rdd, variable etc. and\n# to configure different programming languages respectively.\n\n#from pyspark import SparkContext, SparkConf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4e2682b-7eea-42d3-b069-65b0ff171c1b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# '*' because we have created only one cluster\n# hence we will not have parallel computing\n\n# configuration = SparkConf().setMaster(\"*\").setAppName(\"Rdd_New_Learning\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3653e4a2-0cc8-4180-9953-495f98608c23"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creating oblect of Spark Context \n# this will give error because for one Spark Context we have only one Spark Configuration\n# sc = SparkContext(conf = configuration)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46df2f40-ea09-40d4-a670-4dcc10e10b3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3887087550622300>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Creating oblect of Spark Context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# this will give error because for one Spark Context we have only one Spark Configuration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfiguration\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3887087550622300>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Creating oblect of Spark Context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# this will give error because for one Spark Context we have only one Spark Configuration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfiguration\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "]}}],"execution_count":0},{"cell_type":"code","source":["new_List = [2,3,4,5,6,7,8,9]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"1.Create New RDD using Parallelise Method ","showTitle":true,"inputWidgets":{},"nuid":"5a9849b9-0980-4da5-902c-dbeab9df9bbd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["new_List_Rdd = sc.parallelize(new_List)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22f052ce-3a00-418e-b01b-569b20a2b8fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# here it will show ParallelCollectionRDD[1] means we have one list\nnew_List_Rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce5cecce-7236-4d8e-a77f-5c78044a6fe7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: ParallelCollectionRDD[1] at readRDDFromInputStream at PythonRDD.scala:413","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: ParallelCollectionRDD[1] at readRDDFromInputStream at PythonRDD.scala:413"]}}],"execution_count":0},{"cell_type":"code","source":["# use collect method to see list elements\nnew_List_Rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99299981-6999-4642-8ea8-622d97d4e48a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: [2, 3, 4, 5, 6, 7, 8, 9]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: [2, 3, 4, 5, 6, 7, 8, 9]"]}}],"execution_count":0},{"cell_type":"code","source":["new_List_Rdd.take(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2893c234-b9c3-43da-9736-7e8f58fae76a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: [2, 3, 4]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: [2, 3, 4]"]}}],"execution_count":0},{"cell_type":"code","source":["map_Rdd = new_List_Rdd.map(lambda num : num+5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Map and Lambda functions(2.another way of creating Rdd)","showTitle":true,"inputWidgets":{},"nuid":"5696647d-11d4-49cf-9ec1-8dc9cfe7fb92"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(map_Rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e1b0c7f-5451-4bf7-8ccc-d5361f78941b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: pyspark.rdd.PipelinedRDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: pyspark.rdd.PipelinedRDD"]}}],"execution_count":0},{"cell_type":"code","source":["map_Rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69867549-2415-41df-8abd-acbc0e766e73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: [7, 8, 9, 10, 11, 12, 13, 14]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: [7, 8, 9, 10, 11, 12, 13, 14]"]}}],"execution_count":0},{"cell_type":"code","source":["map_Rdd.filter(lambda X : X%2==0).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Filter","showTitle":true,"inputWidgets":{},"nuid":"14877f5d-e04b-42d9-b160-7135470e835b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[20]: [8, 10, 12, 14]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[20]: [8, 10, 12, 14]"]}}],"execution_count":0},{"cell_type":"code","source":["map_Rdd.map(lambda X : (X,1).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Key-value pair","showTitle":true,"inputWidgets":{},"nuid":"1f095de1-1df2-4133-914f-cf8c5b72942f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# adittion of list\nmap_Rdd.reduce(lambda x,y : x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Operations","showTitle":true,"inputWidgets":{},"nuid":"efc77c5d-ebad-47de-b8c2-e053241e1df3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[31]: 84","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[31]: 84"]}}],"execution_count":0},{"cell_type":"code","source":["map_Rdd.fold(0, lambda x,y : x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93f94c66-fd7c-4335-94e1-06e5303469a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[33]: 84","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[33]: 84"]}}],"execution_count":0},{"cell_type":"code","source":["name_List = ['we belong to spark learning', 'Regex softwares', 'Data Science', 'spark', 'Machine Learning']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"370eb029-98d3-4792-8a68-3949daf30a0b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["name_Rdd = sc.parallelize(name_List)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c741a67-7507-4096-a983-18fbae4fca76"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["name_Rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02402e90-0e79-48a1-8564-63bd3af319ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[42]: ['we belong to spark learning',\n 'Regex softwares',\n 'Data Science',\n 'spark',\n 'Machine Learning']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[42]: ['we belong to spark learning',\n 'Regex softwares',\n 'Data Science',\n 'spark',\n 'Machine Learning']"]}}],"execution_count":0},{"cell_type":"code","source":["name_Rdd.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fc82387-df7b-4d07-8b0a-3a3cee10a5cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[50]: ['we belong to spark learning', 'Regex softwares']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[50]: ['we belong to spark learning', 'Regex softwares']"]}}],"execution_count":0},{"cell_type":"code","source":["name_Rdd.map(lambda x : x.split()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Map Vs Flatmap","showTitle":true,"inputWidgets":{},"nuid":"458c706b-48ed-44ac-bfe2-477fc0f25855"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[61]: [['we', 'belong', 'to', 'spark', 'learning'],\n ['Regex', 'softwares'],\n ['Data', 'Science'],\n ['spark'],\n ['Machine', 'Learning']]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[61]: [['we', 'belong', 'to', 'spark', 'learning'],\n ['Regex', 'softwares'],\n ['Data', 'Science'],\n ['spark'],\n ['Machine', 'Learning']]"]}}],"execution_count":0},{"cell_type":"code","source":["name_Rdd.flatMap(lambda x : x.split()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b139ae03-07d3-45d9-b365-0f6dbfc49aea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# type(arr)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e9198f0-4223-4c75-809c-02e562d7c09c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[59]: list","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[59]: list"]}}],"execution_count":0},{"cell_type":"code","source":["\nblank_Rdd = sc.parallelize([])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Blank Rdd","showTitle":true,"inputWidgets":{},"nuid":"08919ba7-7259-4425-a929-de2f52560d83"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Rdd_Concepts","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3887087550622297}},"nbformat":4,"nbformat_minor":0}
